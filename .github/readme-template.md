# AILIS (AI Layer Interface Specification)

{{ project_description }}

## üìä Project Status

{{ workflow_badges }}

{{ project_stats }}

<!-- TOC_START -->
<!-- TOC_END -->

## üìã Current Proposals

{{ proposal_listing }}

## ü§ù Contributing

{{ contributing_info }}

## üìö Documentation

{{ documentation_links }}

## üéØ Project Goals

AILIS aims to provide a shared mental model for the AI ecosystem, similar to how the OSI model helps us understand networking. Our goals:

- **Clarity**: Common vocabulary for planning and architectural reviews
- **Interoperability**: Clear layer boundaries invite specs and conformance tests  
- **Strategy**: Spot duplication (crowded layers) and white space (under-served layers)
- **Community**: Foster collaboration and discussion around AI system architecture

## üèóÔ∏è The AILIS 16+ Layer Model

We're proposing a 16-layer model (plus cross-cutting concerns) that attempts to map the AI stack from physical infrastructure up through application logic:

### Infrastructure Foundation (L0-L2)
- **L0 ‚Äì Facilities & Power**: Datacenters, power/cooling, physical security
- **L1 ‚Äì Compute Fabric**: GPUs/TPUs/NPUs/CPUs, memory, interconnects  
- **L2 ‚Äì System & Driver Runtime**: CUDA/ROCm/Metal, device memory management

### Model & Inference Stack (L3-L7)
- **L3 ‚Äì ML Graph & Compilation**: XLA/TVM/TensorRT-LLM/ONNX Runtime
- **L4 ‚Äì Numeric & Quantization**: FP16/FP8/INT4, sparsity, calibration
- **L5 ‚Äì Tokenization & Encoders**: BPE tokenizers, CLIP, audio patchifiers
- **L6 ‚Äì Model Parameters & Architecture**: Base/foundation weights, MoE, diffusion
- **L7 ‚Äì Inference Engine & Decoding**: Serving runtimes, caching, speculative decoding

### AI Application Interface (L8-L10)
- **L8 ‚Äì Context Construction & Prompting**: System prompts, templates, few-shot
- **L9 ‚Äì Knowledge & Retrieval**: Vector/graph indexes, rerankers, grounding, citations
- **L10 ‚Äì Tool & Function Invocation**: Typed tool I/O (MCP), function calling, API bindings

### The "Missing Middle" (L8, L11-L15) - *Underserved Today*
- **L8 ‚Äì Context Construction & Prompting**: System prompts, templates, few-shot *(haphazard for enterprises)*
- **L11 ‚Äì Addressing & Registry**: Signed manifests, discovery, capability vectors, fingerprints
- **L12 ‚Äì Routing, Planning & Policy**: Rule DSL + bandits, budgets, privacy, fallback/parallel
- **L13 ‚Äì Transport & Flow Semantics**: Idempotent runs, streaming, CANCEL/RESUME, multiplex
- **L14 ‚Äì Session, Identity & Memory**: Portable session envelope, capability tokens, memory tiers
- **L15 ‚Äì Governance, Safety & Schema**: Redaction, validation/repair, schema change control, audit

### Application Layer (L16)
- **L16 ‚Äì Application & Domain Logic**: Product UX, workflows, agent frameworks

### Cross-Cutting Planes
- **Control**: Policy/configuration management
- **Management/Observability**: Telemetry, evaluations, monitoring
- **Security**: mTLS, key management, PII protection

The proposal suggests that **layers L8 and L11-L15 are particularly underserved** in today's ecosystem. L8 (Context Construction & Prompting) is handled haphazardly by most organizations, while L11-L15 create friction when building multi-provider, privacy-aware, cost-conscious AI systems.

For additional details, see the [AILIS Primer](proposals/AILIS_Primer.md).

---

{{ footer }}